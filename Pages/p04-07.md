
# üî∞ **STAGE 7 ‚Äî Model Evaluation & Validation**

---

## 1Ô∏è‚É£ **Why Evaluate?**

* We don‚Äôt want just a "good-looking forecast."
* We want to measure **how accurate** our model is.
* We test how well the model performs on unseen data.

---

## 2Ô∏è‚É£ **Common Evaluation Metrics**

| Metric                                    | Formula                    | Meaning                     |           |                             |
| ----------------------------------------- | -------------------------- | --------------------------- | --------- | --------------------------- |
| **MAE (Mean Absolute Error)**             | avg(                       | actual - predicted          | )         | Average absolute difference |
| **MSE (Mean Squared Error)**              | avg((actual - predicted)¬≤) | Penalizes large errors      |           |                             |
| **RMSE (Root MSE)**                       | ‚àöMSE                       | More interpretable than MSE |           |                             |
| **MAPE (Mean Absolute Percentage Error)** | avg(                       | actual - predicted          | / actual) | Scaled error (%)            |

---

## 3Ô∏è‚É£ **Simple Train-Test Split**

Let‚Äôs split the data:

```python
# Let‚Äôs assume 80% training and 20% test
train_size = int(len(df['Sales_log']) * 0.8)
train, test = df['Sales_log'].iloc[:train_size], df['Sales_log'].iloc[train_size:]
```

---

## 4Ô∏è‚É£ **Fit Model on Training Data**

```python
from statsmodels.tsa.arima.model import ARIMA

# Example: ARIMA(2,1,2) on training data
model = ARIMA(train, order=(2,1,2))
model_fit = model.fit()

# Forecast the test period
forecast = model_fit.forecast(steps=len(test))

# Inverse log transformation
forecast_orig = np.exp(forecast)
test_orig = np.exp(test)
```

---

## 5Ô∏è‚É£ **Calculate Error Metrics**

```python
from sklearn.metrics import mean_absolute_error, mean_squared_error

# MAE
mae = mean_absolute_error(test_orig, forecast_orig)
# RMSE
rmse = np.sqrt(mean_squared_error(test_orig, forecast_orig))
# MAPE
mape = np.mean(np.abs((test_orig - forecast_orig) / test_orig)) * 100

print(f'MAE: {mae:.2f}')
print(f'RMSE: {rmse:.2f}')
print(f'MAPE: {mape:.2f}%')
```

---

## 6Ô∏è‚É£ **Plot Actual vs Forecast**

```python
plt.figure(figsize=(12,6))
plt.plot(test_orig.index, test_orig, label='Actual')
plt.plot(test_orig.index, forecast_orig, label='Forecast', color='red')
plt.title('Actual vs Forecast on Test Set')
plt.legend()
plt.grid(True)
plt.show()
```

---

‚úÖ **What we achieved:**

* Split data into train/test
* Forecast only on test period
* Calculated MAE, RMSE, MAPE
* Visualized actual vs forecast

---

üëâ **Your task:**

* Try this full evaluation pipeline.
* Test with your **auto\_arima model** as well.
* Compare errors for different models (manual ARIMA vs auto ARIMA vs SARIMA).

---

---

# üî∞ **NEXT STAGE**

‚úÖ You now know how to fully model and evaluate univariate time series.

‚úÖ **Stage 8 will be: Going Beyond ‚Äî Multivariate Time Series (VAR), Exponential Smoothing, and Prophet (Facebook‚Äôs time series package)**

---

